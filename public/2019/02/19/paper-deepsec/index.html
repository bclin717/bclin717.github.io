<!DOCTYPE html><html lang="zh-TW"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="description" content="論文資訊        標題：DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model  作者：Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang, Chunming Wu, Bo Li, Ting Wang  研究機構：Zhejiang Un"><meta name="keywords" content="Tech, Software, Literature"><meta property="og:type" content="article"><meta property="og:title" content="《論文筆記》DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model"><meta property="og:url" content="https:&#x2F;&#x2F;bclin.tw&#x2F;2019&#x2F;02&#x2F;19&#x2F;paper-deepsec&#x2F;index.html"><meta property="og:site_name" content="BC 的日常筆記"><meta property="og:description" content="論文資訊        標題：DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model  作者：Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang, Chunming Wu, Bo Li, Ting Wang  研究機構：Zhejiang Un"><meta property="og:locale" content="zh-TW"><meta property="og:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;DuLKjVN.jpg"><meta property="og:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;WrjeYsN.jpg"><meta property="og:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;pfGeNsO.jpg"><meta property="og:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;IQ7yaux.jpg"><meta property="og:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;ypSORtO.jpg"><meta property="og:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;GlwxLTR.jpg"><meta property="og:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;sDZPYF2.jpg"><meta property="og:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;mcfXM9w.jpg"><meta property="og:updated_time" content="2022-10-04T15:54:05.129Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;imgur.com&#x2F;DuLKjVN.jpg"><title>《論文筆記》DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model | BC 的日常筆記</title><link ref="canonical" href="https://bclin.tw/2019/02/19/paper-deepsec/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun=window.Stun||{},CONFIG={root:"/",algolia:void 0,assistSearch:void 0,fontIcon:{prompt:{success:"fas fa-check-circle",info:"fas fa-arrow-circle-right",warning:"fas fa-exclamation-circle",error:"fas fa-times-circle"},copyBtn:"fas fa-copy"},sidebar:{offsetTop:"20px",tocMaxDepth:6},header:{enable:!0,showOnPost:!0,scrollDownIcon:!1},postWidget:{endText:!0},nightMode:{enable:!0},back2top:{enable:!0},codeblock:{style:"default",highlight:"light",wordWrap:!1},reward:!1,fancybox:!1,zoomImage:{gapAside:"20px"},galleryWaterfall:void 0,lazyload:!1,pjax:{avoidBanner:!0},externalLink:{icon:{enable:!0,name:"fas fa-external-link-alt"}},shortcuts:void 0,prompt:{copyButton:"複製",copySuccess:"複製成功",copyError:"複製失敗"},sourcePath:{js:"js",css:"css",images:"images"}};window.CONFIG=CONFIG</script></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首頁</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">時間軸</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-th"></i></span><span class="header-nav-menu-item__text">分類</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="https://www.notion.so/bclin/8d93520557bc4f498178961836f7305f?v=8ae8644f806544cf8d1d00b8b90f248a" target="_blank" rel="noopener"><span class="header-nav-menu-item__icon"><i class="fas fa-video"></i></span><span class="header-nav-menu-item__text">電影筆記</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="https://www.notion.so/bclin/704a36de4d8f4f34b78b98704eec0af9?v=cd6ea6345adc48f0a425aa3d0d83c66b" target="_blank" rel="noopener"><span class="header-nav-menu-item__icon"><i class="fas fa-book"></i></span><span class="header-nav-menu-item__text">讀書筆記</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user"></i></span><span class="header-nav-menu-item__text">關於 BC.Lin</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">BC 的日常筆記</div><div class="header-banner-info__subtitle">What I cannot create, I do not understand.</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">《論文筆記》DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">撰寫於</span><span class="post-meta-item__value">2019-02-19</span></span><span class="post-meta-item post-meta-item--visitors"><span class="post-meta-item__icon"><i class="fas fa-eye"></i></span><span class="post-meta-item__info">閱讀次數</span><span class="post-meta-item__value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body"><h1 id="論文資訊"><a href="#論文資訊" class="heading-link"><i class="fas fa-link"></i></a><a href="#論文資訊" class="headerlink" title="論文資訊"></a>論文資訊</h1><ul><li>標題：DEEPSEC: A Uniform Platform for Security Analysis of Deep Learning Model</li><li>作者：Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang, Chunming Wu, Bo Li, Ting Wang</li><li>研究機構：Zhejiang University, Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, UIUC, Lehigh University</li><li>會議/期刊：IEEE S&amp;P 2019</li><li>連結：<span class="exturl"><a class="exturl__link" href="https://nesa.zju.edu.cn/download/DEEPSEC%20A%20Uniform%20Platform%20for%20Security%20Analysis%20of%20Deep%20Learning%20Model.pdf" target="_blank" rel="noopener">點我</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> <a id="more"></a><h1 id="Introduction"><a href="#Introduction" class="heading-link"><i class="fas fa-link"></i></a><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1>對於人工智慧界的軟體工程現況，作者提出了幾個問題，第一、即目前對於評估深度學習模型品質的指標過於簡單。例如，誤判率（Misclassification Rate）並無法用來評估一個攻擊方法。<br>第二、之前的論文總是評估一小部分的攻擊或防禦方法。例如，許多防禦方法都只用一些強的攻擊，但其實很多時候能防禦強的攻擊不代表對於弱的攻擊免疫。<br>第三、攻防的快速競爭使得許多方法快速失效。例如，許多防禦方法所採用的 Gradient Obfuscation Strategy 已經沒什麼用了[1]。</li></ul><p>這些因素都會讓研究的結論產生矛盾，像是 Defensive Distillation[2] 原來是用來抵擋 JSMA 攻擊，但現在我們發現其實只對新攻擊的 Marginal Robustness 有增強而已，而且後來的研究指出用上 DD 方法所訓練出的模型表現會比原來的模型還要差[3]。所以說作者認為我們必須要有一套可以泛用，並且提供許多資訊以供評估 Adversarial 攻擊與防禦。而這個平台必須有四個特點：</p><ul><li>Uniform - 這平台可以在相同設定環境下比較不同攻擊與防禦方法</li><li>Comprehensive - 可以提供所有代表性的攻擊與防禦方法</li><li>Informative - 可以給出非常多的指標來評估攻擊與防禦方法</li><li>Extensible - 可以簡單地增加新的攻擊與防禦方法</li></ul><p>而目前有的平台，如 Cleverhans（<span class="exturl"><a class="exturl__link" href="https://github.com/tensorflow/cleverhans" target="_blank" rel="noopener">Github</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>）都沒有達到以上的所有需求。而這篇論文所提出的 DEEPSEC 則包括了 16 種攻擊、10 種攻擊指標、13 種防禦以及 5 種防禦指標。</p><h1 id="Attacks-amp-Defenses"><a href="#Attacks-amp-Defenses" class="heading-link"><i class="fas fa-link"></i></a><a href="#Attacks-amp-Defenses" class="headerlink" title="Attacks &amp; Defenses"></a>Attacks &amp; Defenses</h1><p>關於這篇論文所提出的實驗，都是以白箱攻擊為前提，攻擊者知道所有關於目標模型的細節（深度、節點數、訓練集…之類的），但不知道所部屬的防禦方法。Table 1 給出了所有在這篇論文使用到的攻防方法、評估指標以及其縮寫。<br><a href="https://imgur.com/DuLKjVN.jpg" target="_blank" rel="noopener"><img src="https://imgur.com/DuLKjVN.jpg" alt=""></a></p><h2 id="A-攻擊方法-這篇論文將攻擊方法分為UA（Un-targeted-Attacks）跟TA（Targeted-Attacks），還有-Non-iterative-Attack-與-Iterative-Attack。所以總共有四種攻擊。以上不做敘述。"><a href="#A-攻擊方法-這篇論文將攻擊方法分為UA（Un-targeted-Attacks）跟TA（Targeted-Attacks），還有-Non-iterative-Attack-與-Iterative-Attack。所以總共有四種攻擊。以上不做敘述。" class="heading-link"><i class="fas fa-link"></i></a><a href="#A-攻擊方法-這篇論文將攻擊方法分為UA（Un-targeted-Attacks）跟TA（Targeted-Attacks），還有-Non-iterative-Attack-與-Iterative-Attack。所以總共有四種攻擊。以上不做敘述。" class="headerlink" title="A. 攻擊方法 這篇論文將攻擊方法分為UA（Un-targeted Attacks）跟TA（Targeted Attacks），還有 Non-iterative Attack 與 Iterative Attack。所以總共有四種攻擊。以上不做敘述。"></a>A. 攻擊方法 這篇論文將攻擊方法分為UA（Un-targeted Attacks）跟TA（Targeted Attacks），還有 Non-iterative Attack 與 Iterative Attack。所以總共有四種攻擊。以上不做敘述。</h2><h2 id="B-攻擊的評測指標"><a href="#B-攻擊的評測指標" class="heading-link"><i class="fas fa-link"></i></a><a href="#B-攻擊的評測指標" class="headerlink" title="B. 攻擊的評測指標"></a>B. 攻擊的評測指標</h2><ol><li>Misclassification（誤判）<ol><li>Misclassification Raito（MR 誤判率）：即對抗樣本使分類器誤判的百分比值</li><li>Average Confidence of Adversarial Class（ACAC 平均對抗樣本信心值）：即對抗樣本被成功誤判後的信心值（誤判的確信程度有多高）。</li><li>Average Confidence of True Class（ACTC 平均正確分類信心值）：即對抗樣本的原正確分類之信心值（即離正確分類的距離）。</li></ol></li><li>Imperceptibility（人類無法感知）<ol><li>Average Lp Distortion（ALDp 平均 Lp 扭曲度）：即所有 Lp norm distance （p = 0, 1, infinity）的加總平均值。</li><li>Average Structural Similarity（ASS 平均結構相似性）：即平均對抗樣本與原樣本的 SSIM index 值（SSIM 是一種用以衡量兩張數位影像相似程度的指標）。</li><li>Perturbation Sensitivity Distance（PSD 擾動感知距離）：根據 Contrast Masking Theory [4] 提出的用來衡量人眼對於擾動感知程度的指標 [5]。</li></ol></li><li>Robustness（強健 / 魯棒性）<ol><li>Noise Tolerance Estimation（NTE 雜訊容忍預估）：在[5]這篇論文中，一個對抗樣本的強健性是由 Noise tolerance 預估的，反映了一個對抗樣本可以忍受多大的雜訊而不被破壞。</li><li>Robustness to Gaussian Blur（RGB 高斯模糊耐性）：對抗性樣本可以容忍高斯模糊後處理的比值。</li><li>Robustness to Image Compression（RIC 影像壓縮耐性）：對抗性樣本可以容忍影像壓縮處理的比值。</li></ol></li><li>Computation Cost（CC 計算成本）：即攻擊者產生對抗例樣本的平均時間。</li></ol><h2 id="C-防禦方法"><a href="#C-防禦方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#C-防禦方法" class="headerlink" title="C. 防禦方法"></a>C. 防禦方法</h2><ol><li>Adversarial Training：即拿對抗例去訓練模型，藉此讓模型對對抗例免疫。</li><li>Gradient Masking/Regularization：減少模型對對抗例的敏感度，並且隱藏梯度。</li><li>Input Transformation：將 input 樣本做轉換，以破壞惡意擾動。</li><li>Region-based Classification：[6]（看不太懂）</li><li>Detection-only Defenses：把對抗例分類對實在太難了，我們只要可以「檢測」出對抗例，並且拒絕接收就好。</li></ol><h2 id="D-防禦的評測指標：防禦可以從兩個角度來評估，「功能保留」以及「抵抗攻擊」。"><a href="#D-防禦的評測指標：防禦可以從兩個角度來評估，「功能保留」以及「抵抗攻擊」。" class="heading-link"><i class="fas fa-link"></i></a><a href="#D-防禦的評測指標：防禦可以從兩個角度來評估，「功能保留」以及「抵抗攻擊」。" class="headerlink" title="D. 防禦的評測指標：防禦可以從兩個角度來評估，「功能保留」以及「抵抗攻擊」。"></a>D. 防禦的評測指標：防禦可以從兩個角度來評估，「功能保留」以及「抵抗攻擊」。</h2><ol><li>Classification Accuracy Variance（CAV 分類精準度變異量）：增強防禦後的模型分類一般測試集的精準度與原模型的差異值。</li><li>Classification Rectify/Sacrifice Ratio（CRR/CSR 分類矯正比/分類犧牲比）：增強防禦後的模型分類一般測試集的時候，原來分錯變成分對的就是 CRR；分對變成分錯的就是 CSR。</li><li>Classification Confidence Variance（CCV 分類信心值變異量）：增強防禦後的模型可能不會影響精準度，但信心值會有所波動，CCV 就是原來的信心值和增強防禦後的信心值之平均比值。</li><li>Classification Output Stability（COS 分類輸出穩定值）：利用 JS divergence[7] 去計算輸出機率的相似度。</li></ol><h1 id="System-Design-And-Implementation"><a href="#System-Design-And-Implementation" class="heading-link"><i class="fas fa-link"></i></a><a href="#System-Design-And-Implementation" class="headerlink" title="System Design And Implementation"></a>System Design And Implementation</h1><p><img src="https://imgur.com/WrjeYsN.jpg" alt=""></p><h2 id="A-System-Design：分為五個部分"><a href="#A-System-Design：分為五個部分" class="heading-link"><i class="fas fa-link"></i></a><a href="#A-System-Design：分為五個部分" class="headerlink" title="A. System Design：分為五個部分"></a>A. System Design：分為五個部分</h2><ol><li>Attack Module（AM）：製作對抗例去攻擊深度學習模型，共 16 種攻擊，一半是 UA，一半 TA。</li><li>Defense Module（DM）：防禦深度學習模型，共 13 種防禦。</li><li>Attack Utility Evaluation（AUE）：10 種評估攻擊的指標。</li><li>Defense Utility Evaluation（DUE）：5 種評估防禦的指標</li><li>Security Evaluation（SE）：平衡 AM 和 DM 的安全性指標，可以讓使用者知道這個模型抵抗對抗例的能力如何。</li></ol><h2 id="B-System-Implementation"><a href="#B-System-Implementation" class="heading-link"><i class="fas fa-link"></i></a><a href="#B-System-Implementation" class="headerlink" title="B. System Implementation"></a>B. System Implementation</h2><h1 id="Evaluations"><a href="#Evaluations" class="heading-link"><i class="fas fa-link"></i></a><a href="#Evaluations" class="headerlink" title="Evaluations"></a>Evaluations</h1><p>平台： Intel Xeon 2.2GHz CPU x 2 256GB RAM GTX 1080 x 1</p><h2 id="A-Evaluation-of-Attacks"><a href="#A-Evaluation-of-Attacks" class="heading-link"><i class="fas fa-link"></i></a><a href="#A-Evaluation-of-Attacks" class="headerlink" title="A. Evaluation of Attacks"></a>A. Evaluation of Attacks</h2><ol><li>Experimental Setup<ol><li>Datasets：MNIST / CIFAR-10</li><li>Mode：7-layer CNN / ResNet-20</li><li>Accuracy：99.27% / 85.95%</li><li>首先挑了 1000 個可以被正確分類的 sample，然後針對每一種攻擊都生成 1000 張對抗例，再用評估方法對所有攻擊做評估。TA 的 Class 則是隨機選擇但是統一的。</li><li>參數部分都統一。</li></ol></li><li>Experiemntal Results</li></ol><p><img src="https://imgur.com/pfGeNsO.jpg" alt=""></p><ol><li>Misclassification：Iterative 強於 Non-iterative<ul><li>Remark 1：大部分狀況下，目前的所有攻擊都有非常高的成功率（以 MR 來看的話，Iterative 的所有攻擊方式，在 CIFAR-10 上都有接近 100% 的 MR），都能很有效的誤導目標模型。有個有趣的現象是，如果對抗例的 ACTC 比較低，那對於攻擊不同模型時較具有泛用性。</li><li>即存在一種狀況，若分類後 真正的 class 信心值很低，其他 class 的信心值無論高低，那這個泛用性就很高；若真正的 class 信心值很高，假 class 只是相對低一點，那泛用性就不高。直接比較 ACAC 不準，因為參數不同，例如 ILLC （L無限攻擊）跟 CW2（L2 攻擊）攻擊是不同的，要看 ACTC 的值。</li><li>例子：FGSM 跟 OM 產生的對抗例在 CIFAR-10 都有 0.75 的 ACAC，但是 FGSM 的 ACTC 卻是 OM 的六倍低，這代表 FGSM 的泛用性會比 OM 更高。</li></ul></li><li>Imperceptibility：PSD 很敏感，但是量測 L2 攻擊產生的對抗例時，數值都比 L1 和 L無限 小很多。<ul><li>Remark 2：在所有指標中，PSD 是對對抗例擾動的 Imperceptibility 最敏感的，ASS 則相反，是最不敏感的，所以不建議用來量化對伉例。在這一個測試裡面也發現誤判率和 Imperceptibility 是有確定的 trade-off 關係的。</li></ul></li><li>Robustness：利用了 Guetzli 去產生高品質的 jpg 壓縮檔，RGB 指標用的樣本，高斯模糊的 radius 是 0.5；RIC 指標用的則是設為 90% 的壓縮率。測試後發現， NTE 比 RGB 跟 RIC 更有意義。可能是因為越高的 NTE 表示了容忍的雜訊越多，根據 Imperceptibility 的結論，雜訊增加意味著誤判機率越高，所以比低 NTE 更能忍受影像轉換。另一方面， NTE、RGB、RIC 之間的關係是非線性的。<ol><li>ACAC 越高，RIC 跟 RGB 就越高。是因為 ACAC 可以影響 NTE，而間接影響了 RGB 跟 RIC。</li><li>Remark 3：對抗例的強健性是由 ACAC 影響的。另外 UA 比 TA 強健性更高。</li></ol></li><li>Computation Cost：設定保持不變，計算了各種攻擊方法的平均對抗例產生時間，iterative 通常比 non-iterative 慢很多，多花了超過十倍以上的時間。</li></ol><h2 id="B-Evaluation-of-Defenses"><a href="#B-Evaluation-of-Defenses" class="heading-link"><i class="fas fa-link"></i></a><a href="#B-Evaluation-of-Defenses" class="headerlink" title="B. Evaluation of Defenses"></a>B. Evaluation of Defenses</h2><ol><li>Experimental Setup</li><li>Results</li></ol><p><img src="https://imgur.com/IQ7yaux.jpg" alt=""></p><ul><li>NAT、DD、TE、RC 在 MNIST 跟 CIFAR-10 上面的精準度並沒有受到什麼影響。</li><li>IGR-、RT-、和 PD 強化過的模型的精準度則在 CIFAR-10 上受到很大的影響。</li><li>如果準確率下降，CSR 多數大於 CRR。</li><li>部屬防禦後 CCV 都會受到影響，如 PAT 和 IGR 則特別嚴重。這是因為部屬後的模型變得不穩定。</li><li>COS 跟 CAV 的變化傾向是相同的。</li><li>Remark 4：大部分指標都還滿有效的。</li></ul><h2 id="C-Defenses-vs-Attacks"><a href="#C-Defenses-vs-Attacks" class="heading-link"><i class="fas fa-link"></i></a><a href="#C-Defenses-vs-Attacks" class="headerlink" title="C. Defenses vs. Attacks"></a>C. Defenses vs. Attacks</h2><ol><li>Complete Defenses：將對抗例正確辨識出來。<img src="https://imgur.com/ypSORtO.jpg" alt=""><ul><li>Results<ul><li>大部分防禦方法都有用，但有大有小。像是 NAT 對所有攻擊都有 80% 以上的抵擋能力，而所有方法平均是 58.4%。</li><li>抵擋 TA 比 UA 還有用。作者猜測是因為 UA 只是讓模型誤判，泛用性比較強；而 TA 則通常是針對模型去做攻擊，所以用了防禦後模型不同，效果就會打折扣。</li></ul></li><li>Remark 5：多數防禦方法都有比較適合的攻擊方法，而通常有 retrain 模型的比沒有的強。</li></ul></li><li>Detection：偵測出對抗例並拒絕輸出。<img src="https://imgur.com/GlwxLTR.jpg" alt=""><ul><li>Experiemental Setup<ul><li>先找了 4-A 成功被辨識錯誤的對抗例</li><li>再從測試集裡面隨機挑選可以被正確辨識的測試樣本</li></ul></li><li>Results<ul><li>AUC：當你隨機挑選一個正樣本以及一個負樣本，當前的分類算法根據計算得到的Score值將這個正樣本排在負樣本前面的概率就是AUC值。</li><li>所有偵測方法的效果都不錯，平均 AUC 都有高於 70% 的表現。</li><li>LID 中的 AUC 平均表現最好，但再面對 DF 或 OM 都只有 65%，比 FS 和 MagNet 還低（平均高於 80%）。</li><li>LID 的 TPR 表現最好，但是 FS 和 MagNet 在 MNIST 表現比較好，只是在 CIFAR-10 比較差。可能是因為參數調整的問題。</li><li>比較高的擾動不一定比較容易被偵測到。</li></ul></li><li>Remark 6：所有的偵測方法對目前的攻擊方法都有一定效果。不同的偵測方法有適合他們的攻擊方法。而高的擾動不一定比較容易被偵測。</li></ul></li></ol><h1 id="Case-Studies"><a href="#Case-Studies" class="heading-link"><i class="fas fa-link"></i></a><a href="#Case-Studies" class="headerlink" title="Case Studies"></a>Case Studies</h1><h2 id="A-Case-Study-1-Transferability-of-Adversarial-Attacks"><a href="#A-Case-Study-1-Transferability-of-Adversarial-Attacks" class="heading-link"><i class="fas fa-link"></i></a><a href="#A-Case-Study-1-Transferability-of-Adversarial-Attacks" class="headerlink" title="A. Case Study 1: Transferability of Adversarial Attacks"></a>A. Case Study 1: Transferability of Adversarial Attacks</h2><p><img src="https://imgur.com/sDZPYF2.jpg" alt=""></p><p>主要為了測試不同攻擊的遷移性。</p><ol><li>Experimental Setup：訓練了三個模型要來進行測試<ul><li>Model 1：一樣的模型，但經歷了不同的隨機初始化。</li><li>Model 2：一樣的模型設定，但是神經網路結構有些微不同。</li><li>Model 3：完全不同的模型。</li></ul></li><li>Results<ul><li>個種攻擊方法或多或少都有一定的遷移性</li><li>對抗例在其他模型的 ACAC 高於原模型，作者猜測原因是原來信心值很低的對抗例會遷移失敗，而遷移成功的通常是信心值很高的對抗例。</li><li>UA 的遷移性高於 TA，在 CIFAR-10 測試集，UA 平均有 74.6%，TA 僅僅 10%。</li><li>L無限 攻擊遷移性強過 L2 或 L0。</li><li>Remark 7：不同的攻擊有不同的遷移性<ul><li>UA 遷移性強過 TA</li><li>L無限 遷移性強過其他攻擊</li><li>對抗例在其他模型的 ACAC 高於原模型</li></ul></li></ul></li></ol><h2 id="B-Case-Study-2-Is-Ensemble-of-Defense-More-Robust"><a href="#B-Case-Study-2-Is-Ensemble-of-Defense-More-Robust" class="heading-link"><i class="fas fa-link"></i></a><a href="#B-Case-Study-2-Is-Ensemble-of-Defense-More-Robust" class="headerlink" title="B. Case Study 2: Is Ensemble of Defense More Robust"></a>B. Case Study 2: Is Ensemble of Defense More Robust</h2><p><img src="https://imgur.com/mcfXM9w.jpg" alt=""></p><p>不同防禦手法組合後抵禦攻擊的能力是否會增強？</p><ol><li>Experiment Setup：和 4A 用的測試方法一樣。並有三種防禦組合：<ul><li>Completely-random Ensemble：隨機從九種防禦中選擇三種組合</li><li>Interclass-random Ensemble：從三種不同類別的防禦方法中各隨機挑選一種進行組合</li><li>Best-defense Ensemble：從前面的 4-C1 測試中表現最好的前三名進行組合，對 MNIST 用 PAT、TE、NAT；對 CIFAR-10 則是 NAT、EIT、EAT。</li></ul></li><li>Results<ul><li>Remark 8：把防禦方法都組合起來不會比較有效，但可以增加 lower bound。這部分跟之前看過的[8]一樣。</li></ul></li></ol><h1 id="Reference"><a href="#Reference" class="heading-link"><i class="fas fa-link"></i></a><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,” in ICML, 2018.<br>[2] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation as a defense to adversarial perturbations against deep neural networks,” in S&amp;P, 2016.<br>[3] A. S. Ross and F. Doshi-Velez, “Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients,” in AAAI, 2018.<br>[4] A. Liu, W. Lin, M. Paul, C. Deng, and F. Zhang, “Just noticeable difference for images with decomposition model for separating edge and textured regions,” IEEE Transactions on Circuits and Systems for Video Technology, 2010.<br>[5] B. Luo, Y. Liu, L. Wei, and Q. Xu, “Towards imperceptible and robust adversarial example attacks against neural networks,” in AAAI, 2018.<br>[6] X. Cao and N. Z. Gong, “Mitigating evasion attacks to deep neural networks via region-based classification,” in ACSAC, 2017.<br>[7] Wikipedia, “Jensen–shannon divergence,” <span class="exturl"><a class="exturl__link" href="https://en.wikipedia.org/wiki/Jensen-Shannon" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Jensen-Shannon</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> divergence.<br>[8] W. He, J. Wei, X. Chen, N. Carlini, and D. Song, “Adversarial example defense: Ensembles of weak defenses are not strong,” in WOOT, 2017.</p></div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">本文結束，感謝您的閱讀</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://bclin.tw">BC Lin</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文連結: </span><span class="copyright-link__value"><a href="https://bclin.tw/2019/02/19/paper-deepsec/">https://bclin.tw/2019/02/19/paper-deepsec/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版權聲明: </span><span class="copyright-notice__value">本部落格所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh_tw" rel="external nofollow" target="_blank">BY-NC-SA</a> 許可協議。轉載請註明出處！</span></div></div><div class="post-share"><div class="social-share" data-sites="linkedin, facebook, twitter, google">Share to:</div></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2019/02/25/when-we-were-orphans/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">時代的潮流下，誰人不是毫無依靠？談石黑一雄《我輩孤雛》</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2019/02/09/chronicle-of-a-death-foretold/"><span class="paginator-prev__text">巧合與荒謬，閱讀馬奎斯《預知死亡紀事》</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目錄</span><span class="sidebar-nav-ov">站點概覽</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#論文資訊"><span class="toc-number">1.</span> <span class="toc-text">論文資訊</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Attacks-amp-Defenses"><span class="toc-number">3.</span> <span class="toc-text">Attacks &amp; Defenses</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#A-攻擊方法-這篇論文將攻擊方法分為UA（Un-targeted-Attacks）跟TA（Targeted-Attacks），還有-Non-iterative-Attack-與-Iterative-Attack。所以總共有四種攻擊。以上不做敘述。"><span class="toc-number">3.1.</span> <span class="toc-text">A. 攻擊方法 這篇論文將攻擊方法分為UA（Un-targeted Attacks）跟TA（Targeted Attacks），還有 Non-iterative Attack 與 Iterative Attack。所以總共有四種攻擊。以上不做敘述。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#B-攻擊的評測指標"><span class="toc-number">3.2.</span> <span class="toc-text">B. 攻擊的評測指標</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#C-防禦方法"><span class="toc-number">3.3.</span> <span class="toc-text">C. 防禦方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#D-防禦的評測指標：防禦可以從兩個角度來評估，「功能保留」以及「抵抗攻擊」。"><span class="toc-number">3.4.</span> <span class="toc-text">D. 防禦的評測指標：防禦可以從兩個角度來評估，「功能保留」以及「抵抗攻擊」。</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#System-Design-And-Implementation"><span class="toc-number">4.</span> <span class="toc-text">System Design And Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#A-System-Design：分為五個部分"><span class="toc-number">4.1.</span> <span class="toc-text">A. System Design：分為五個部分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#B-System-Implementation"><span class="toc-number">4.2.</span> <span class="toc-text">B. System Implementation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Evaluations"><span class="toc-number">5.</span> <span class="toc-text">Evaluations</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Evaluation-of-Attacks"><span class="toc-number">5.1.</span> <span class="toc-text">A. Evaluation of Attacks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#B-Evaluation-of-Defenses"><span class="toc-number">5.2.</span> <span class="toc-text">B. Evaluation of Defenses</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#C-Defenses-vs-Attacks"><span class="toc-number">5.3.</span> <span class="toc-text">C. Defenses vs. Attacks</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Case-Studies"><span class="toc-number">6.</span> <span class="toc-text">Case Studies</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Case-Study-1-Transferability-of-Adversarial-Attacks"><span class="toc-number">6.1.</span> <span class="toc-text">A. Case Study 1: Transferability of Adversarial Attacks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#B-Case-Study-2-Is-Ensemble-of-Defense-More-Robust"><span class="toc-number">6.2.</span> <span class="toc-text">B. Case Study 2: Is Ensemble of Defense More Robust</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-number">7.</span> <span class="toc-text">Reference</span></a></li></ol></section><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/avatar.png" alt="avatar"></div></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/bclin717" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://www.linkedin.com/in/bckevinlin/" target="_blank" rel="noopener" data-popover="social.linkedin" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-linkedin"></i></span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">45</div><div class="sidebar-ov-state-item__name">時間軸</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">9</div><div class="sidebar-ov-state-item__name">分類</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh_tw" target="_blank" rel="noopener" data-popover="知識共享許可協議" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已閱讀了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2022</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>BCLin All Rights Reserved.</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 強力驅動</span><span> v4.0.0</span><span class="footer__devider">|</span><span>主題 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__info">訪客總數</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__info">總瀏覽次數</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zindex="-1"></script><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener("DOMContentLoaded",function(){new Pjax({selectors:["head title","#main",".pjax-reload"],history:!0,scrollTo:!1,scrollRestoration:!1,cacheBust:!1,debug:!1,currentUrlFullReload:!1,timeout:0});var e=null;document.addEventListener("pjax:send",function(){$(".header-nav-menu").removeClass("show"),CONFIG.pjax&&CONFIG.pjax.avoidBanner&&$("html").velocity("scroll",{duration:500,offset:$("#header").height(),easing:"easeInOutCubic"});var a=20;$(".loading-bar").addClass("loading"),$(".loading-bar__progress").css("width",a+"%"),clearInterval(e),e=setInterval(function(){95<(a+=3)&&(a=95),$(".loading-bar__progress").css("width",a+"%")},500)},!1),window.addEventListener("pjax:complete",function(){clearInterval(e),$(".loading-bar__progress").css("width","100%"),$(".loading-bar").removeClass("loading"),setTimeout(function(){$(".loading-bar__progress").css("width","0")},400),$("link[rel=prefetch], script[data-pjax-rm]").each(function(){$(this).remove()}),$("script[data-pjax], #pjax-reload script").each(function(){$(this).parent().append($(this).remove())}),Stun.utils.pjaxReloadBoot&&Stun.utils.pjaxReloadBoot(),Stun.utils.pjaxReloadScroll&&Stun.utils.pjaxReloadScroll(),Stun.utils.pjaxReloadSidebar&&Stun.utils.pjaxReloadSidebar()},!1)},!1)</script><div id="pjax-reload"><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script></div><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>